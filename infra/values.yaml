# Airflow Helm Chart Configuration
# Official Apache Airflow Helm Chart Values
# https://artifacthub.io/packages/helm/apache-airflow/airflow

# Airflow Docker image
defaultAirflowRepository: apache/airflow
defaultAirflowTag: 2.10.3-python3.11
airflowVersion: 2.10.3

# Airflow executor configuration
executor: KubernetesExecutor

# Environment variables for all Airflow containers
env:
  - name: AIRFLOW__CORE__LOAD_EXAMPLES
    value: "false"
  - name: AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION
    value: "true"
  # StatsD Configuration
  - name: AIRFLOW__METRICS__STATSD_ON
    value: "true"
  - name: AIRFLOW__METRICS__STATSD_HOST
    value: "airflow-statsd-exporter"
  - name: AIRFLOW__METRICS__STATSD_PORT
    value: "9125"
  - name: AIRFLOW__METRICS__STATSD_PREFIX
    value: "airflow"
  # OpenTelemetry Configuration
  - name: AIRFLOW__METRICS__OTEL_ON
    value: "true"
  - name: AIRFLOW__METRICS__OTEL_HOST
    value: "otel-collector"
  - name: AIRFLOW__METRICS__OTEL_PORT
    value: "4318"
  - name: AIRFLOW__METRICS__OTEL_PREFIX
    value: "airflow"
  - name: AIRFLOW__METRICS__OTEL_INTERVAL_MILLISECONDS
    value: "30000"
  - name: AIRFLOW__METRICS__OTEL_SSL_ACTIVE
    value: "false"
  # Tracing Configuration
  - name: AIRFLOW__TRACES__OTEL_ON
    value: "false"
  - name: AIRFLOW__TRACES__OTEL_HOST
    value: "otel-collector"
  - name: AIRFLOW__TRACES__OTEL_PORT
    value: "4318"
  - name: AIRFLOW__TRACES__OTEL_TASK_LOG_EVENT
    value: "true"
  # Scheduler Configuration
  - name: AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK
    value: "true"
  - name: AIRFLOW__METRICS__METRICS_USE_PATTERN_MATCH
    value: "false"

# Airflow database - PostgreSQL
postgresql:
  enabled: true
  auth:
    username: airflow
    password: airflow
    database: airflow
  persistence:
    enabled: true
    size: 10Gi

# Redis for Celery backend (if needed)
redis:
  enabled: false

# Airflow webserver configuration
webserver:
  replicas: 2
  resources:
    requests:
      memory: "512Mi"
      cpu: "250m"
    limits:
      memory: "1Gi"
      cpu: "500m"
  service:
    type: NodePort
    port: 8080

  # Liveness and readiness probes
  livenessProbe:
    enabled: true
    initialDelaySeconds: 60
    periodSeconds: 30
  readinessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 10

# Airflow scheduler configuration
scheduler:
  replicas: 2
  resources:
    requests:
      memory: "512Mi"
      cpu: "250m"
    limits:
      memory: "1Gi"
      cpu: "500m"

  # Enable scheduler health check
  livenessProbe:
    enabled: true
    initialDelaySeconds: 60
    periodSeconds: 30

# Airflow triggerer configuration
triggerer:
  enabled: true
  replicas: 1
  resources:
    requests:
      memory: "256Mi"
      cpu: "125m"
    limits:
      memory: "512Mi"
      cpu: "250m"

# GitSync configuration for DAGs
dags:
  gitSync:
    enabled: true
    repo: https://github.com/samuel-aka-viana/airflow-prometheus.git
    branch: main
    rev: HEAD
    depth: 1
    maxFailures: 0
    subPath: "dags"
    wait: 60
    containerName: git-sync
    uid: 65533

    # Resources for git-sync container
    resources:
      requests:
        memory: "64Mi"
        cpu: "50m"
      limits:
        memory: "128Mi"
        cpu: "100m"

# Logs configuration
logs:
  persistence:
    enabled: true
    size: 10Gi
    storageClassName: standard

# Ingress configuration
ingress:
  enabled: true
  web:
    enabled: true
    ingressClassName: nginx
    annotations: {}
    hosts:
      - name: airflow.local.test
        tls:
          enabled: false
    path: /
# Security context
securityContext:
  runAsUser: 50000
  runAsGroup: 50000
  fsGroup: 50000

# Pod annotations for Prometheus scraping
#podAnnotations:
#  prometheus.io/scrape: "true"
#  prometheus.io/port: "8080"
#  prometheus.io/path: "/metrics"

# Service Account
serviceAccount:
  create: true
  name: airflow

# RBAC configuration
rbac:
  create: true
  createSCCRoleBinding: false

# Airflow users configuration
users:
  - username: admin
    password: admin
    role: Admin
    email: admin@example.com
    firstName: Admin
    lastName: User

# Extra pip packages to install
extraPipPackages:
  - apache-airflow[otel]==2.10.3
  - apache-airflow[statsd]==2.10.3

# Config for KubernetesExecutor
config:
  core:
    dags_folder: /opt/airflow/dags
    load_examples: false
    executor: KubernetesExecutor

  kubernetes:
    namespace: airflow
    delete_worker_pods: true
    delete_worker_pods_on_failure: false
    worker_container_repository: apache/airflow
    worker_container_tag: 2.10.3-python3.11

  kubernetes_executor:
    namespace: airflow
    delete_worker_pods: true

  webserver:
    expose_config: true

  scheduler:
    statsd_on: true
    statsd_host: airflow-statsd-exporter
    statsd_port: 9125
    statsd_prefix: airflow

# Cleanup configuration
cleanup:
  enabled: true
  schedule: "*/15 * * * *"

# Flower configuration (for monitoring if using CeleryExecutor)
flower:
  enabled: false

# StatsD configuration (to be deployed separately)
statsd:
  enabled: false  # We'll deploy our own StatsD exporter

# Prometheus configuration
prometheus:
  enabled: false  # We'll deploy our own Prometheus

# Grafana configuration
grafana:
  enabled: false  # We'll deploy our own Grafana